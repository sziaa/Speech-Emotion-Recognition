{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7396491c-08ac-4dbc-afc4-a98cd422ae1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.10.2.post1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from librosa) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from librosa) (1.11.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from librosa) (1.3.0)\n",
      "Requirement already satisfied: joblib>=0.14 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from librosa) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from librosa) (0.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from librosa) (4.5.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from librosa) (1.0.8)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from lazy-loader>=0.1->librosa) (23.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pooch>=1.1->librosa) (3.10.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pooch>=1.1->librosa) (2.31.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from soundfile>=0.12.1->librosa) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2023.7.22)\n",
      "Requirement already satisfied: soundfile in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.12.1)\n",
      "Requirement already satisfied: cffi>=1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from soundfile) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.24.3)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\n",
      "zsh:1: command not found: brew\n",
      "Collecting pyaudio\n",
      "  Using cached PyAudio-0.2.14.tar.gz (47 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: pyaudio\n",
      "  Building wheel for pyaudio (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for pyaudio \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[16 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-universal2-cpython-311\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-10.9-universal2-cpython-311/pyaudio\n",
      "  \u001b[31m   \u001b[0m copying src/pyaudio/__init__.py -> build/lib.macosx-10.9-universal2-cpython-311/pyaudio\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building 'pyaudio._portaudio' extension\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-10.9-universal2-cpython-311\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-10.9-universal2-cpython-311/src\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-10.9-universal2-cpython-311/src/pyaudio\n",
      "  \u001b[31m   \u001b[0m clang -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch arm64 -arch x86_64 -g -DMACOS=1 -I/usr/local/include -I/usr/include -I/opt/homebrew/include -I/Library/Frameworks/Python.framework/Versions/3.11/include/python3.11 -c src/pyaudio/device_api.c -o build/temp.macosx-10.9-universal2-cpython-311/src/pyaudio/device_api.o\n",
      "  \u001b[31m   \u001b[0m clang: error: sh -c '/Applications/Xcode.app/Contents/Developer/usr/bin/xcodebuild -sdk macosx -find clang 2> /dev/null' failed with exit code 17920: (null) (errno=No such file or directory)\n",
      "  \u001b[31m   \u001b[0m xcode-select: Failed to locate 'clang', requesting installation of command line developer tools.\n",
      "  \u001b[31m   \u001b[0m error: command '/usr/bin/clang' failed with exit code 72\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for pyaudio\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hFailed to build pyaudio\n",
      "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (pyaudio)\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#Code Source: https://data-flair.training/blogs/python-mini-project-speech-emotion-recognition/#goog_rewarded\n",
    "#Install necessary packages\n",
    "!pip install librosa \n",
    "!pip install soundfile \n",
    "!pip install numpy \n",
    "!pip install scikit-learn \n",
    "!brew install portaudio\n",
    "!pip install pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ca3f2b8-2efa-47a8-b1ef-b45442f28f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages:\n",
    "\n",
    "#analyzes audio and music files\n",
    "import librosa\n",
    "\n",
    "#reading and writing sound files\n",
    "import soundfile\n",
    "\n",
    "#os allows for interacting with operating systems; glob allows for searching for pathname patterns; pickle allows for serializing and deserializing Python objects\n",
    "import os, glob, pickle\n",
    "\n",
    "#numerical calcutions\n",
    "import numpy as np\n",
    "\n",
    "#Splits model into data sets to train and test (ML)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#multi layer percpton classifier -> type of feedforward neural network used for learning tasks\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#computes accuracy of the classified model\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0847aabe-7658-4e96-b2a4-3632bb05221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function extract to pull feature data from a sound file. \n",
    "#Inputs of file path and 3 boolean values of mfcc, chroma, mel\n",
    "      #MFCC: Mel Frequency Cepstral Coefficient, represents the short-term power spectrum of a sound (timbral features reflecting how humans percive frequencies)\n",
    "      #Chroma: 12 different pitch classes of music (C,C#,D,D#..) -> tonal component\n",
    "      #Mel: Mel Spectrogram Frequency, power of the signal across different frequencies (Visualizes the energy distribution across frequencies on a mel scale)\n",
    "#Ouputs: if any of the 3 booleans are true extract the mean value of it\n",
    "\n",
    "def extract(file_name, mfcc, chroma, mel):\n",
    "    #open soundfile\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "\n",
    "        #read audio data into an array and convert to float32 type\n",
    "        X = sound_file.read(dtype = \"float32\")\n",
    "\n",
    "        #get sample rate (how frequently the analog audio signal is sampled converted into digital form)\n",
    "        sample_rate = sound_file.samplerate\n",
    "\n",
    "        #initalize an empty array to store results (mean values of 3 features outlined above)\n",
    "        result = np.array([])\n",
    "\n",
    "        #if chroma feature extracted is enable, compute short time fourier transformation of the audio data\n",
    "        # STFT  maps a signal into a two-dimensional function of time and frequency\n",
    "        if chroma: \n",
    "            stft = np.abs(librosa.stft(X))\n",
    "\n",
    "        # if mfcc is true, compute the mfcc's and take the mean. horizontally stack the result into result array\n",
    "        if mfcc: \n",
    "            mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "            results = np.hstack((result,mfccs))\n",
    "\n",
    "\n",
    "        #if chroma is true, compute chroma features from stft and take mean. horizontally stack the result into result array\n",
    "        if chroma: \n",
    "            chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis = 0)\n",
    "            result = np.hstack((result, chroma))\n",
    "\n",
    "        #if mel is true, compute mel spectrogram from data and take mean. horizontally stack the result into result array\n",
    "        if mel: \n",
    "            mel = np.mean(librosa.feature.melspectrogram(y= X, sr=sample_rate).T,axis = 0)\n",
    "            result = np.hstack((result, mel))\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82858029-aab9-4861-b61f-ccbab9b87357",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comprehensive dictonary of all emotions detectable in the data set:\n",
    "emotions={\n",
    "  '01':'neutral',\n",
    "  '02':'calm',\n",
    "  '03':'happy',\n",
    "  '04':'sad',\n",
    "  '05':'angry',\n",
    "  '06':'fearful',\n",
    "  '07':'disgust',\n",
    "  '08':'surprised'}\n",
    "\n",
    "#list of emotions we are looking to observe:\n",
    "observed_emotions=['calm', 'happy', 'fearful', 'disgust']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af0ac19e-bf4b-42fd-aca1-15aa5109a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in data and extract features for each sound file depending on if that file has an emotion we want to observe indicated thorught the dictonary defined above\n",
    "#Input: relative size of the test size\n",
    "def load_data(test_size=0.2):\n",
    "    \n",
    "    #initalize empty lists for features (x) and lables (y) of the sound files we want to consider\n",
    "    x, y = [],[]\n",
    "\n",
    "\n",
    "    #loop through all WAV files in directory using glob (relative path in my case)\n",
    "    for file in glob.glob(\"SER/Actor_*/*.wav\"):\n",
    "        #get file name\n",
    "        file_name = os.path.basename(file)\n",
    "    \n",
    "        #Extract the emotion label from the file name (assuming the corresponding emotion is in the file name)\n",
    "        emotion = emotions[file_name.split(\"-\")[2]]\n",
    "    \n",
    "        #if the file does not include an emotion we want to observe (observed_emotions list), skip file\n",
    "        if emotion not in observed_emotions:\n",
    "            continue\n",
    "    \n",
    "        #OW extract mfcc, chroma and mel features from sound file \n",
    "        feature = extract(file, mfcc = True, chroma= True, mel = True)\n",
    "    \n",
    "        #Append the extracted features to the x list\n",
    "        x.append(feature)\n",
    "        \n",
    "        #Append the correpsonding emotion to the y list\n",
    "        y.append(emotion)\n",
    "\n",
    "    #split data into training and testing test for machine learning\n",
    "    return train_test_split(np.array(x), y, test_size = test_size, random_state = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "544cbc70-bb0d-45ae-9344-95eccf1a6098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(576, 192)\n",
      "Features extracted: 140\n"
     ]
    }
   ],
   "source": [
    "#Split data into sets for training and testing; we will use 25% of data for testing and 75% for training\n",
    "x_train,x_test,y_train,y_test=load_data(test_size=0.25)\n",
    "\n",
    "# Observe the shape of the training and testing datasets\n",
    "print((x_train.shape[0], x_test.shape[0]))\n",
    "\n",
    "# Observe the number of features extracted\n",
    "print(f'Features extracted: {x_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "83a0a7d1-88f5-47e6-a1d4-59656f6fad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the MLP Classifier that optimizes the log-loss function\n",
    "    #Alpha: L2 regulariztion term that prevents overfitting by penalizin large weights in the model (larger alphja has stronger regularization)\n",
    "    #batchsize: number of training samples used in one iteration\n",
    "    #epsilon: added to denominator of graident descent to prevent divison of 0\n",
    "    #hidden_layer_sizes = hidden layer architecture (gere we have 1 hiden layer w 300 neaurons)\n",
    "    #learning_rate: adaptive means the rate decreases only when model stops improving\n",
    "    #max_iter: maximum epcohs(number of iterations) for model training \n",
    "model=MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08, hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=800)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2caf8b0f-4d94-48fd-8e44-961ec6789446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-12 {color: black;}#sk-container-id-12 pre{padding: 0;}#sk-container-id-12 div.sk-toggleable {background-color: white;}#sk-container-id-12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-12 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-12 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-12 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-12 div.sk-item {position: relative;z-index: 1;}#sk-container-id-12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-12 div.sk-item::before, #sk-container-id-12 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-12 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-12 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-12 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-12 div.sk-label-container {text-align: center;}#sk-container-id-12 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-12 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(alpha=0.01, batch_size=256, hidden_layer_sizes=(300,),\n",
       "              learning_rate=&#x27;adaptive&#x27;, max_iter=800)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" checked><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(alpha=0.01, batch_size=256, hidden_layer_sizes=(300,),\n",
       "              learning_rate=&#x27;adaptive&#x27;, max_iter=800)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(alpha=0.01, batch_size=256, hidden_layer_sizes=(300,),\n",
       "              learning_rate='adaptive', max_iter=800)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train model to learn patterns:\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6f74110f-9606-4ced-a37e-102019323e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the values for the test sets (using x_test)\n",
    "y_pred=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "37a5c6ff-9973-4799-b4a5-627408e13aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 60.94%\n"
     ]
    }
   ],
   "source": [
    "#Calculate accuracy of the model by comparing y_pred and y_test\n",
    "accuracy=accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "#print result\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1c84cf40-b05f-4e60-aaec-c5a165d5ede2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess user-uploaded audio files; ie pull out mcff, chroma, mel\n",
    "def preprocess_user_audio(file_path):\n",
    "    \n",
    "    # Extract features from the audio file using the same method as in training\n",
    "    feature = extract(file_path, mfcc=True, chroma=True, mel=True)\n",
    "    \n",
    "    # Return the features in the correct shape for model input\n",
    "    return np.array([feature])\n",
    "\n",
    "# Function to predict emotion from an audio file\n",
    "def predict_emotion(file_path):\n",
    "    \n",
    "    # Preprocess the audio file to extract features\n",
    "    features = preprocess_user_audio(file_path)\n",
    "    \n",
    "    # Use the trained model to predict the emotion (outputs a list of probabilities)\n",
    "    #emotion_probabilities = model.predict(features)\n",
    "    emotion_probabilities = model.predict_proba(features)[0]\n",
    "\n",
    "    #print(emotion_probabilities)\n",
    "\n",
    "    # Find the index of the highest probability\n",
    "    highest_probable_index = np.argmax(emotion_probabilities)\n",
    "\n",
    "    # Convert the index to the corresponding emotion label using the 'emotions' dictionary\n",
    "    predicted_emotion = emotions[list(emotions.keys())[highest_probable_index]]\n",
    "\n",
    "    # Return the predicted emotion\n",
    "    return predicted_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3133853f-965f-429e-99aa-db26812a44fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the path to the audio file:  test_audio.wav\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted emotion: sad\n"
     ]
    }
   ],
   "source": [
    "# Command-line interface (CLI) for user interaction\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Prompt the user to enter the path to an audio file\n",
    "    file_path = input(\"Enter the path to the audio file: \")\n",
    "    \n",
    "    # Predict the emotion from the provided audio file\n",
    "    emotion = predict_emotion(file_path)\n",
    "    \n",
    "    # Print the predicted emotion\n",
    "    print(f\"Predicted emotion: {emotion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a3dec4-fbe7-4fca-a95a-d442b50de4f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
